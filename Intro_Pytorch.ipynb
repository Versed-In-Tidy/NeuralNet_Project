{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**ATTENTION!**</font>\n",
    "\n",
    "This material was gathered from the Medium article **[An easy introduction to Pytorch for Neural Networks](https://towardsdatascience.com/an-easy-introduction-to-pytorch-for-neural-networks-3ea08516bff2)** and is for replication purposes only. I do not claim the rights to this Medium article nor do I take responsibility for the author's work. This Jupyter example has been created for educational purposes only. Credit goes to the original author, **[George Seif](https://medium.com/@george.seif94)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, the development of Pytorch was aimed at being as similar to Python’s Numpy as possible. Doing so would allow an easy and smooth interaction between regular Python code, Numpy, and Pytorch allowing for faster and easier coding.\n",
    "\n",
    "To get started, we can install Pytorch via pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’re interested in looking at specific features, the [Pytorch docs](https://pytorch.org/docs/stable/index.html) are amazing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load our packages needed for this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "In PyTorch, tensors can be declared using the simple Tensor object. The below code creates a tensor of size (3, 3) — i.e. 3 rows and 3 columns, filled with floating point zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(3, 3)\n",
    "print(x)\n",
    "print(\"\")\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create tensors filled random floating point values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5344, 0.8970, 0.2451],\n",
      "        [0.6515, 0.4329, 0.3200],\n",
      "        [0.0344, 0.1011, 0.1795]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying tensors, adding them, and other basic math is super easy with Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3,3)\n",
    "y = torch.ones(3,3) * 4\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even Numpy-like slicing functions are available with Pytorch tensors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3,3) * 5\n",
    "y = x[:, :3]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Pytorch tensors can very much be used and worked with in the same way as Numpy arrays. Now we’ll look at how we can build Deep Networks with these easy Pytorch tensors as our building blocks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Neural Networks with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Pytorch, neural networks are defined as Python classes. The class which defines the network extends the torch.nn.Module from the Torch library. Let’s create a class for a Convolutional Neural Network (CNN) which we’ll apply on the MNIST dataset.\n",
    "Check out the code below which defines our network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=(3, 3), padding=1)\n",
    "        self.max_pool = nn.MaxPool2d(2, 2)\n",
    "        self.global_pool = nn.AvgPool2d(7)\n",
    "        self.fc1 = nn.Linear(64, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.global_pool(x)\n",
    "\n",
    "        x = x.view(-1, 64)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = F.log_softmax(x)\n",
    "\n",
    "        return x\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most important functions in a Pytorch network class are the *__* *init* *__()* and the *forward()* functions. The *__* *init* *__()* is used to define any network layers that your model will use. The *forward()* function is where you actually set up the model by stacking all the layers together.\n",
    "\n",
    "For our model, we’ve defined 2 convolutional layers in the init function, one of which we’ll re-use a few times (conv2). We have a max-pooling layer and a global average pooling layer to be applied near the end. Finally we have our Full-Connected (FC) layers and a softmax to get the final output probabilities.\n",
    "\n",
    "In the forward function, we define exactly how our layers stack up together to form the full model. It’s a standard network with stacked conv, pooling, and FC layers. The beauty of Pytorch is that we can print out the shape and result of any tensor within the intermediate layers with just a simple print statement wherever you want in the *forward()* function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, Testing, and Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading up data\n",
    "\n",
    "Time to get our data ready for training! We’ll starting but getting the necessary imports ready, initialise parameters, and making sure Pytorch is setup to use the GPU. The line below which uses `torch.device()` checks if Pytorch was installed with CUDA support and if so uses the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the MNIST dataset straight from Pytroch. We’ll download the data and put the train and test sets into separate tensors. Once that data is loaded, we’ll pass it to a torch *DataLoader* which just gets it ready to pass to the model with a specific batch size and optional shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='data',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data',\n",
    "                                          train=False,\n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "It’s training time!\n",
    "\n",
    "The optimzer (we’ll use Adam) and loss function (we’ll use cross entropy) are defined quite similarly to other deep learning libraries like TensorFlow, Keras, and MXNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pytorch, all network models and datasets much be explicitly transferred from CPU to GPU. We do this by applying the `.to()` function to our model below. Later, we’ll do the same for our image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can write out our training loop. Check out the code below to see how it works!\n",
    "\n",
    "1. All Pytorch training loops will go through each epoch and each batch in the training data loader.\n",
    "2. On each loop iteration, the image data and labels are transferred to the GPU.\n",
    "3. Each training loop also explicitly applies the forward pass, backward pass, and optimisation steps.\n",
    "4. The model is applied to the images in the batch and then the loss for that batch is calculated.\n",
    "5. The gradients are calculated and back-propagated through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djcoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/1875], Loss: 2.2637\n",
      "Epoch [1/10], Step [200/1875], Loss: 1.4102\n",
      "Epoch [1/10], Step [300/1875], Loss: 0.8213\n",
      "Epoch [1/10], Step [400/1875], Loss: 0.4231\n",
      "Epoch [1/10], Step [500/1875], Loss: 0.4085\n",
      "Epoch [1/10], Step [600/1875], Loss: 0.4400\n",
      "Epoch [1/10], Step [700/1875], Loss: 0.3668\n",
      "Epoch [1/10], Step [800/1875], Loss: 0.5550\n",
      "Epoch [1/10], Step [900/1875], Loss: 0.1462\n",
      "Epoch [1/10], Step [1000/1875], Loss: 0.2231\n",
      "Epoch [1/10], Step [1100/1875], Loss: 0.2215\n",
      "Epoch [1/10], Step [1200/1875], Loss: 0.2195\n",
      "Epoch [1/10], Step [1300/1875], Loss: 0.6095\n",
      "Epoch [1/10], Step [1400/1875], Loss: 0.2616\n",
      "Epoch [1/10], Step [1500/1875], Loss: 0.2854\n",
      "Epoch [1/10], Step [1600/1875], Loss: 0.1278\n",
      "Epoch [1/10], Step [1700/1875], Loss: 0.0089\n",
      "Epoch [1/10], Step [1800/1875], Loss: 0.0954\n",
      "Epoch [2/10], Step [100/1875], Loss: 0.1180\n",
      "Epoch [2/10], Step [200/1875], Loss: 0.1079\n",
      "Epoch [2/10], Step [300/1875], Loss: 0.2683\n",
      "Epoch [2/10], Step [400/1875], Loss: 0.0686\n",
      "Epoch [2/10], Step [500/1875], Loss: 0.0315\n",
      "Epoch [2/10], Step [600/1875], Loss: 0.1214\n",
      "Epoch [2/10], Step [700/1875], Loss: 0.0280\n",
      "Epoch [2/10], Step [800/1875], Loss: 0.2401\n",
      "Epoch [2/10], Step [900/1875], Loss: 0.1490\n",
      "Epoch [2/10], Step [1000/1875], Loss: 0.1163\n",
      "Epoch [2/10], Step [1100/1875], Loss: 0.0904\n",
      "Epoch [2/10], Step [1200/1875], Loss: 0.0522\n",
      "Epoch [2/10], Step [1300/1875], Loss: 0.0970\n",
      "Epoch [2/10], Step [1400/1875], Loss: 0.0146\n",
      "Epoch [2/10], Step [1500/1875], Loss: 0.0777\n",
      "Epoch [2/10], Step [1600/1875], Loss: 0.0294\n",
      "Epoch [2/10], Step [1700/1875], Loss: 0.0206\n",
      "Epoch [2/10], Step [1800/1875], Loss: 0.1679\n",
      "Epoch [3/10], Step [100/1875], Loss: 0.0026\n",
      "Epoch [3/10], Step [200/1875], Loss: 0.4825\n",
      "Epoch [3/10], Step [300/1875], Loss: 0.1945\n",
      "Epoch [3/10], Step [400/1875], Loss: 0.0058\n",
      "Epoch [3/10], Step [500/1875], Loss: 0.0093\n",
      "Epoch [3/10], Step [600/1875], Loss: 0.1553\n",
      "Epoch [3/10], Step [700/1875], Loss: 0.0959\n",
      "Epoch [3/10], Step [800/1875], Loss: 0.1237\n",
      "Epoch [3/10], Step [900/1875], Loss: 0.0276\n",
      "Epoch [3/10], Step [1000/1875], Loss: 0.1144\n",
      "Epoch [3/10], Step [1100/1875], Loss: 0.0038\n",
      "Epoch [3/10], Step [1200/1875], Loss: 0.0018\n",
      "Epoch [3/10], Step [1300/1875], Loss: 0.0576\n",
      "Epoch [3/10], Step [1400/1875], Loss: 0.1375\n",
      "Epoch [3/10], Step [1500/1875], Loss: 0.2085\n",
      "Epoch [3/10], Step [1600/1875], Loss: 0.1125\n",
      "Epoch [3/10], Step [1700/1875], Loss: 0.0372\n",
      "Epoch [3/10], Step [1800/1875], Loss: 0.0195\n",
      "Epoch [4/10], Step [100/1875], Loss: 0.0032\n",
      "Epoch [4/10], Step [200/1875], Loss: 0.0122\n",
      "Epoch [4/10], Step [300/1875], Loss: 0.0300\n",
      "Epoch [4/10], Step [400/1875], Loss: 0.0144\n",
      "Epoch [4/10], Step [500/1875], Loss: 0.0695\n",
      "Epoch [4/10], Step [600/1875], Loss: 0.0018\n",
      "Epoch [4/10], Step [700/1875], Loss: 0.0171\n",
      "Epoch [4/10], Step [800/1875], Loss: 0.0263\n",
      "Epoch [4/10], Step [900/1875], Loss: 0.1079\n",
      "Epoch [4/10], Step [1000/1875], Loss: 0.0044\n",
      "Epoch [4/10], Step [1100/1875], Loss: 0.0031\n",
      "Epoch [4/10], Step [1200/1875], Loss: 0.0022\n",
      "Epoch [4/10], Step [1300/1875], Loss: 0.0127\n",
      "Epoch [4/10], Step [1400/1875], Loss: 0.1350\n",
      "Epoch [4/10], Step [1500/1875], Loss: 0.0137\n",
      "Epoch [4/10], Step [1600/1875], Loss: 0.0123\n",
      "Epoch [4/10], Step [1700/1875], Loss: 0.2639\n",
      "Epoch [4/10], Step [1800/1875], Loss: 0.0076\n",
      "Epoch [5/10], Step [100/1875], Loss: 0.0940\n",
      "Epoch [5/10], Step [200/1875], Loss: 0.1948\n",
      "Epoch [5/10], Step [300/1875], Loss: 0.0095\n",
      "Epoch [5/10], Step [400/1875], Loss: 0.2007\n",
      "Epoch [5/10], Step [500/1875], Loss: 0.0740\n",
      "Epoch [5/10], Step [600/1875], Loss: 0.0595\n",
      "Epoch [5/10], Step [700/1875], Loss: 0.0089\n",
      "Epoch [5/10], Step [800/1875], Loss: 0.0579\n",
      "Epoch [5/10], Step [900/1875], Loss: 0.0186\n",
      "Epoch [5/10], Step [1000/1875], Loss: 0.0058\n",
      "Epoch [5/10], Step [1100/1875], Loss: 0.0165\n",
      "Epoch [5/10], Step [1200/1875], Loss: 0.0065\n",
      "Epoch [5/10], Step [1300/1875], Loss: 0.0010\n",
      "Epoch [5/10], Step [1400/1875], Loss: 0.0451\n",
      "Epoch [5/10], Step [1500/1875], Loss: 0.0260\n",
      "Epoch [5/10], Step [1600/1875], Loss: 0.0180\n",
      "Epoch [5/10], Step [1700/1875], Loss: 0.1188\n",
      "Epoch [5/10], Step [1800/1875], Loss: 0.0386\n",
      "Epoch [6/10], Step [100/1875], Loss: 0.0375\n",
      "Epoch [6/10], Step [200/1875], Loss: 0.0528\n",
      "Epoch [6/10], Step [300/1875], Loss: 0.0639\n",
      "Epoch [6/10], Step [400/1875], Loss: 0.0117\n",
      "Epoch [6/10], Step [500/1875], Loss: 0.0046\n",
      "Epoch [6/10], Step [600/1875], Loss: 0.0968\n",
      "Epoch [6/10], Step [700/1875], Loss: 0.0036\n",
      "Epoch [6/10], Step [800/1875], Loss: 0.0005\n",
      "Epoch [6/10], Step [900/1875], Loss: 0.0327\n",
      "Epoch [6/10], Step [1000/1875], Loss: 0.0108\n",
      "Epoch [6/10], Step [1100/1875], Loss: 0.0303\n",
      "Epoch [6/10], Step [1200/1875], Loss: 0.0032\n",
      "Epoch [6/10], Step [1300/1875], Loss: 0.0370\n",
      "Epoch [6/10], Step [1400/1875], Loss: 0.0571\n",
      "Epoch [6/10], Step [1500/1875], Loss: 0.0975\n",
      "Epoch [6/10], Step [1600/1875], Loss: 0.0553\n",
      "Epoch [6/10], Step [1700/1875], Loss: 0.0014\n",
      "Epoch [6/10], Step [1800/1875], Loss: 0.0097\n",
      "Epoch [7/10], Step [100/1875], Loss: 0.0000\n",
      "Epoch [7/10], Step [200/1875], Loss: 0.0092\n",
      "Epoch [7/10], Step [300/1875], Loss: 0.0514\n",
      "Epoch [7/10], Step [400/1875], Loss: 0.1283\n",
      "Epoch [7/10], Step [500/1875], Loss: 0.0232\n",
      "Epoch [7/10], Step [600/1875], Loss: 0.0049\n",
      "Epoch [7/10], Step [700/1875], Loss: 0.2077\n",
      "Epoch [7/10], Step [800/1875], Loss: 0.0446\n",
      "Epoch [7/10], Step [900/1875], Loss: 0.0023\n",
      "Epoch [7/10], Step [1000/1875], Loss: 0.0079\n",
      "Epoch [7/10], Step [1100/1875], Loss: 0.0003\n",
      "Epoch [7/10], Step [1200/1875], Loss: 0.0071\n",
      "Epoch [7/10], Step [1300/1875], Loss: 0.0006\n",
      "Epoch [7/10], Step [1400/1875], Loss: 0.1787\n",
      "Epoch [7/10], Step [1500/1875], Loss: 0.1525\n",
      "Epoch [7/10], Step [1600/1875], Loss: 0.2227\n",
      "Epoch [7/10], Step [1700/1875], Loss: 0.1991\n",
      "Epoch [7/10], Step [1800/1875], Loss: 0.0028\n",
      "Epoch [8/10], Step [100/1875], Loss: 0.0040\n",
      "Epoch [8/10], Step [200/1875], Loss: 0.0049\n",
      "Epoch [8/10], Step [300/1875], Loss: 0.0400\n",
      "Epoch [8/10], Step [400/1875], Loss: 0.0113\n",
      "Epoch [8/10], Step [500/1875], Loss: 0.0165\n",
      "Epoch [8/10], Step [600/1875], Loss: 0.0107\n",
      "Epoch [8/10], Step [700/1875], Loss: 0.0173\n",
      "Epoch [8/10], Step [800/1875], Loss: 0.0588\n",
      "Epoch [8/10], Step [900/1875], Loss: 0.2494\n",
      "Epoch [8/10], Step [1000/1875], Loss: 0.0323\n",
      "Epoch [8/10], Step [1100/1875], Loss: 0.0004\n",
      "Epoch [8/10], Step [1200/1875], Loss: 0.0149\n",
      "Epoch [8/10], Step [1300/1875], Loss: 0.0045\n",
      "Epoch [8/10], Step [1400/1875], Loss: 0.0064\n",
      "Epoch [8/10], Step [1500/1875], Loss: 0.2018\n",
      "Epoch [8/10], Step [1600/1875], Loss: 0.1033\n",
      "Epoch [8/10], Step [1700/1875], Loss: 0.0300\n",
      "Epoch [8/10], Step [1800/1875], Loss: 0.0152\n",
      "Epoch [9/10], Step [100/1875], Loss: 0.0023\n",
      "Epoch [9/10], Step [200/1875], Loss: 0.0946\n",
      "Epoch [9/10], Step [300/1875], Loss: 0.0162\n",
      "Epoch [9/10], Step [400/1875], Loss: 0.0308\n",
      "Epoch [9/10], Step [500/1875], Loss: 0.0640\n",
      "Epoch [9/10], Step [600/1875], Loss: 0.0263\n",
      "Epoch [9/10], Step [700/1875], Loss: 0.0021\n",
      "Epoch [9/10], Step [800/1875], Loss: 0.0414\n",
      "Epoch [9/10], Step [900/1875], Loss: 0.0001\n",
      "Epoch [9/10], Step [1000/1875], Loss: 0.0111\n",
      "Epoch [9/10], Step [1100/1875], Loss: 0.0027\n",
      "Epoch [9/10], Step [1200/1875], Loss: 0.0090\n",
      "Epoch [9/10], Step [1300/1875], Loss: 0.0607\n",
      "Epoch [9/10], Step [1400/1875], Loss: 0.0002\n",
      "Epoch [9/10], Step [1500/1875], Loss: 0.0444\n",
      "Epoch [9/10], Step [1600/1875], Loss: 0.1795\n",
      "Epoch [9/10], Step [1700/1875], Loss: 0.0058\n",
      "Epoch [9/10], Step [1800/1875], Loss: 0.0377\n",
      "Epoch [10/10], Step [100/1875], Loss: 0.0005\n",
      "Epoch [10/10], Step [200/1875], Loss: 0.1105\n",
      "Epoch [10/10], Step [300/1875], Loss: 0.0285\n",
      "Epoch [10/10], Step [400/1875], Loss: 0.0002\n",
      "Epoch [10/10], Step [500/1875], Loss: 0.0369\n",
      "Epoch [10/10], Step [600/1875], Loss: 0.0003\n",
      "Epoch [10/10], Step [700/1875], Loss: 0.0085\n",
      "Epoch [10/10], Step [800/1875], Loss: 0.0039\n",
      "Epoch [10/10], Step [900/1875], Loss: 0.0157\n",
      "Epoch [10/10], Step [1000/1875], Loss: 0.0028\n",
      "Epoch [10/10], Step [1100/1875], Loss: 0.0003\n",
      "Epoch [10/10], Step [1200/1875], Loss: 0.0024\n",
      "Epoch [10/10], Step [1300/1875], Loss: 0.0066\n",
      "Epoch [10/10], Step [1400/1875], Loss: 0.0013\n",
      "Epoch [10/10], Step [1500/1875], Loss: 0.0016\n",
      "Epoch [10/10], Step [1600/1875], Loss: 0.1169\n",
      "Epoch [10/10], Step [1700/1875], Loss: 0.0006\n",
      "Epoch [10/10], Step [1800/1875], Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Saving\n",
    "\n",
    "Testing a network’s performance in Pytorch sets up a similar loop as in the training phase. The main difference being that we don’t need to do a backward propagation of the gradients. We’ll still do the forward-pass and just get the label with the maximum probability at the output of the network.\n",
    "\n",
    "In this case, after 10 epochs our network got an accuracy of 99.16% on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djcoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the MNIST test images: 99.16 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the MNIST test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the model to disk to use later, just use the `torch.save()` function and voila!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
